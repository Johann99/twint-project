{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# pd.set_option(\"display.max_columns\", 999)\n",
    "\n",
    "# NLTK VADER for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# granger causality analysis \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# source: https://github.com/twintproject/twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure\n",
    "c = twint.Config()\n",
    "\n",
    "# tesla & TSLA | &zm | $ual | $siri | $pcar | $nvda | $wba | $sbux\n",
    "c.Search = \"tsla\"\n",
    "# c.Since = '2021-01-01'\n",
    "# c.Until = '2021-01-03'\n",
    "\n",
    "# c.Debug = True\n",
    "# c.Limit = 10\n",
    "\n",
    "# problematic because most people do not show where their geolocation is --> data loss\n",
    "# c.Near = \"NYC\"\n",
    "# c.Geo=\"40.730610,-73.935242,500km\" \n",
    "\n",
    "c.Language = \"en\"\n",
    "\n",
    "\n",
    "# filter verified: in order to solve the data abundance problem (too much data -> too much time -> a lot of noise in the data) I turned on verification. This leads to a skew towards newspaper and \"famous\" people. \n",
    "c.Verified = False\n",
    "\n",
    "## filter popular tweets - 1.Option: min_likes, min_retweets, min_replies - 2.Option: popular-tweets (pt)\n",
    "# c.Min_likes = 10\n",
    "# c.Min_retweets = 10\n",
    "# problem with 10 -> 12 -> 9\n",
    "# minimum replies eliminates most of the noise from the data, since usually bots or scammers do not comment much - it's easier to like or retweet \n",
    "# c.Min_replies = 5\n",
    "\n",
    "# not working!\n",
    "# c.Pt = True\n",
    "\n",
    "# not working!\n",
    "# c.Filter_retweets = False\n",
    "\n",
    "\n",
    "\n",
    "# c.Stats = True\n",
    "\n",
    "\n",
    "c.Store_json = True\n",
    "# c.Custom_json = [\"id\", \"user_id\", \"created_at\", \"date\", \"tweet\", \"timezone\", \"hashtags\"]\n",
    "c.Output = \"/Users/pietj.ginski/Desktop/BWL-Studium/BWL 6 Semester/Bachelor Thesis/Raw Data BT/archive-2/test.json\"\n",
    "\n",
    "\n",
    "# Run - this is commmented out!!\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# *** NLP Function ***\n",
    "\n",
    "def nlp(data):\n",
    "\n",
    "# create the analyzer \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# prepare the data\n",
    "    sentences = np.array(data[\"tweet\"])\n",
    "\n",
    "# filter out retweets - we need individual thoughts without any \"pre influence\"\n",
    "    data = data[data['retweet']== False]\n",
    "\n",
    "# get the vader scores \n",
    "    data['vader_scores'] = data[\"tweet\"].apply(lambda sentences: analyzer.polarity_scores(sentences))\n",
    "\n",
    "# append vader_scores \n",
    "    data['compound'] = data['vader_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "    \n",
    "# create delta_compound \n",
    "    data[\"delta_compound\"] = (data[\"compound\"] / data[\"compound\"].shift(1)) - 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  *** Engagement Score Function*** \n",
    "\n",
    "def engagement_score(df):\n",
    "# creating a sensible engagement score through normalization\n",
    "    x = df[['replies_count','retweets_count','likes_count']].values \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df_2 = pd.DataFrame(x_scaled)\n",
    "    engagement_score = df_2[0]+df_2[1]+df_2[2]\n",
    "\n",
    "# append engagement_score & compound_engagement_score\n",
    "    df['engagement_score'] = engagement_score\n",
    "    df['compound_engagement_score'] = df['engagement_score']*df['compound']\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# *** Stock Data Function ***\n",
    "\n",
    "def yahoo(ticker, start_date, end_date, df_name):\n",
    "\n",
    "# define the ticker symbol\n",
    "    tickerSymbol = ticker\n",
    "\n",
    "# get data on this ticker\n",
    "    tickerData = yf.Ticker(tickerSymbol)\n",
    "\n",
    "# get the historical prices for this ticker\n",
    "    df_name = tickerData.history(period='1d', start=start_date, end=end_date)\n",
    "\n",
    "# get daily returns\n",
    "    df_name[\"daily_returns\"] = (df_name[\"Close\"] / df_name[\"Close\"].shift(1)) - 1\n",
    "\n",
    "# convert index to date_s colums\n",
    "    df_name['date_s'] = pd.to_datetime(df_name.index)\n",
    "    \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Group Function ***\n",
    "\n",
    "def group(data, stock_data):\n",
    "\n",
    "    data_g = data.groupby(pd.to_datetime(data['date']).dt.date).mean()\n",
    "\n",
    "# convert to datetime\n",
    "    data_g['date_s'] = pd.to_datetime(data_g.index)\n",
    "\n",
    "# # merge the two datasets\n",
    "    data_g_merge = pd.merge(data_g, stock_data, on='date_s', how='inner')\n",
    "    \n",
    "    return data_g_merge\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Correlation Analysis Function ***\n",
    "\n",
    "def correlation(data,column_1,column_2):\n",
    "\n",
    "    corr, p_value = scipy.stats.pearsonr(data[column_1][1:], data[column_2][1:])\n",
    "\n",
    "    print('Correlation Coefficient:',corr.round(3))\n",
    "    print('P-Value:',p_value.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Lag Function *** \n",
    "\n",
    "def lag(df):\n",
    "\n",
    "# apple delta return (has to be one day ahead )\n",
    "    ap_dr = df[\"Close\"].shift(2)[3:]\n",
    "# apple delta compound \n",
    "    ap_dc = df[\"compound\"][3:]\n",
    "\n",
    "\n",
    "# correlation analysis\n",
    "    corr, p_value = scipy.stats.pearsonr(ap_dc, ap_dr)\n",
    "\n",
    "    print('Correlation Coefficient:',corr.round(3))\n",
    "    print('P-Value:',p_value.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Granger Causality Function ***\n",
    "\n",
    "def granger(data, lag):\n",
    "\n",
    "    data_2 = data[[\"Close\", \"compound\"]].pct_change().dropna()\n",
    "    data_2.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "    data_2 = data_2.dropna()\n",
    "# # execute granger causality test\n",
    "    gc_res = grangercausalitytests(data_2, lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Dataset \n",
    "\n",
    "# file name \n",
    "file_name = \"twint_tesla_verified.json\"\n",
    "string = \"/Users/pietj.ginski/Desktop/BWL-Studium/BWL 6 Semester/Bachelor Thesis/Raw Data BT/archive-2/rubbish/{}\".format(file_name)\n",
    "# import the json file \n",
    "raw_msft = pd.read_json(string, lines = True)\n",
    "\n",
    "raw_msft.count()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply nlp()\n",
    "nlp_msft = nlp(raw_msft)\n",
    "\n",
    "# apply yahoo()\n",
    "price_msft = yahoo('MSFT', '2020-06-01','2021-05-31', 'msft')\n",
    "\n",
    "# apply group()\n",
    "grouped_msft = group(nlp_msft, price_msft)\n",
    "\n",
    "# apply engagement_score() \n",
    "grouped_msft = engagement_score(grouped_msft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
